{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwhC18DMoB7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "\n",
        "def unet(input_shape, num_classes):\n",
        "    inputs = Input(input_shape)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "\n",
        "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
        "    merge5 = concatenate([conv3, up5], axis=3)\n",
        "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(merge5)\n",
        "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n",
        "\n",
        "    up6 = UpSampling2D(size=(2, 2))(conv5)\n",
        "    merge6 = concatenate([conv2, up6], axis=3)\n",
        "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(merge6)\n",
        "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = UpSampling2D(size=(2, 2))(conv6)\n",
        "    merge7 = concatenate([conv1, up7], axis=3)\n",
        "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(merge7)\n",
        "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)\n",
        "\n",
        "    output_layer = Conv2D(num_classes, (1, 1), activation='sigmoid')(conv7)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output_layer)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define input shape, number of classes, and batch size\n",
        "input_shape = (256, 256, 1)\n",
        "num_classes = 1\n",
        "batch_size = 16\n",
        "\n",
        "# Load dataset\n",
        "train_ds = (...path_to_dataset)\n",
        "val_ds = (...path_to_dataset)\n",
        "test_ds = (...path_to_dataset)\n",
        "\n",
        "# Preprocess dataset\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.Resizing(input_shape[0], input_shape[1]),\n",
        "  tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (resize_and_rescale(x, training=True), y))\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = val_ds.map(lambda x, y: (resize_and_rescale(x, training=False), y))\n",
        "val_ds = val_ds.batch(batch_size)\n",
        "val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "test\n",
        "# Initialize the U-Net model\n",
        "model = unet(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss=BinaryCrossentropy(),\n",
        "              metrics=['accuracy', MeanIoU(num_classes=num_classes)])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_ds,\n",
        "                    validation_data=val_ds,\n",
        "                    epochs=50,  # Modify this based on your requirement\n",
        "                    batch_size=batch_size)\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy, test_mean_iou = model.evaluate(test_ds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "print(f\"Test Mean IoU: {test_mean_iou}\")\n",
        "\n",
        "# Optionally, save the model\n",
        "model.save('path/to/save/model')\n"
      ],
      "metadata": {
        "id": "b_eumYM8oB9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def load_and_preprocess_image(image_path, input_shape):\n",
        "    # Load image\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize(input_shape[:2])  # Assuming input_shape is like (height, width, channels)\n",
        "    image = np.array(image)\n",
        "    image = image / 255.0  # Normalize the image\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    return image\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('path/to/saved/model')\n",
        "\n",
        "# Load and preprocess new images\n",
        "# Example path to a new image\n",
        "new_image_path = 'path/to/new/image.jpg'\n",
        "input_shape = (256, 256, 1)  # Same as used during training\n",
        "preprocessed_image = load_and_preprocess_image(new_image_path, input_shape)\n",
        "\n",
        "# Predict tissue mask\n",
        "predicted_mask = model.predict(preprocessed_image)\n"
      ],
      "metadata": {
        "id": "eL0Lg9zSo-Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_mask(mask, save_path, image_name):\n",
        "    # Convert the mask to an image\n",
        "    mask_image = Image.fromarray((mask[0, :, :, 0] * 255).astype('uint8'))\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    mask_image.save(os.path.join(save_path, image_name))\n",
        "\n",
        "# Example save path\n",
        "save_path = './predicted_tissue_masks'\n",
        "image_name = 'predicted_mask.jpg'\n",
        "\n",
        "# Save the predicted mask\n",
        "save_mask(predicted_mask, save_path, image_name)\n"
      ],
      "metadata": {
        "id": "JZUEYqbQo_G1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}